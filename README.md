<div align="center">

# ğŸ§¬ MuSAE-Inv

### Multi-layer Sparse Autoencoder Invariant Causal Feature Selection<br>for Cross-Domain Hallucination Detection

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.1+](https://img.shields.io/badge/pytorch-2.1+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![arXiv](https://img.shields.io/badge/arXiv-2025.xxxxx-b31b1b.svg)](#citation)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

<p align="center">
  <img src="docs/assets/architecture.png" alt="MuSAE-Inv Architecture" width="800">
</p>

*MuSAE-Inv extracts monosemantic features from Sparse Autoencoders (SAEs) across multiple Transformer layers, selects causally invariant features via a novel counterfactual-based ICFS criterion, and trains a lightweight L1-regularised logistic regression probe that generalises across domains without retraining.*

</div>

---

## Table of Contents

- [Overview](#overview)
- [Key Results](#key-results)
- [Architecture](#architecture)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Datasets](#datasets)
- [Full Pipeline](#full-pipeline)
- [Configuration](#configuration)
- [Ablation Studies](#ablation-studies)
- [Project Structure](#project-structure)
- [Reproducibility](#reproducibility)
- [Hardware Requirements](#hardware-requirements)
- [Citation](#citation)
- [License](#license)
- [Acknowledgements](#acknowledgements)

---

## Overview

Large Language Models (LLMs) hallucinate â€” they produce fluent but factually incorrect text. Detecting these hallucinations is critical, but most existing detectors are **domain-specific**: they work well on the domain they were trained on but degrade sharply on out-of-distribution (OOD) text.

**MuSAE-Inv** solves this by:

1. **Sparse Autoencoder (SAE) Feature Extraction** â€” We pass inputs through [Gemma-2-2B](https://huggingface.co/google/gemma-2-2b) and extract monosemantic features from [Gemma Scope SAEs](https://huggingface.co/google/gemma-scope-2b-pt-res-canonical) at layers {6, 12, 18, 25}, capturing both syntactic and semantic representations.

2. **Invariant Causal Feature Selection (ICFS v2)** â€” We generate counterfactual (true, hallucinated) text pairs and compute per-feature causal effect scores using a `min(|CE_QA|, |CE_Dial|) Ã— sign_consistent` criterion that selects features invariant across QA and Dialogue domains.

3. **Domain-Augmented L1-LR Probe** â€” A logistic regression probe trained on the 512-dimensional ICFS feature vector (128 features Ã— 4 layers) with combined QA + Dialogue training data and L1 regularisation (`C=0.3`) for sparsity.

4. **Zero-Shot Transfer** â€” The probe transfers to unseen domains (Summarisation, TruthfulQA) without any domain-specific fine-tuning.

### Why SAEs?

Standard hidden states are **polysemantic** â€” each neuron responds to multiple unrelated concepts, entangling hallucination signals with domain-specific artefacts. SAEs disentangle these into **monosemantic features**: each learned direction corresponds to a single interpretable concept. This makes the invariance selection more effective and the resulting probe more transferable.

---

## Key Results

### Main Results (AUROC %)

| Method | QA (ID) | Dialogue (OOD) | Summ (OOD) | TruthfulQA (OOD) |
|:---|:---:|:---:|:---:|:---:|
| Random | 50.00 | 50.00 | 50.00 | 50.00 |
| SelfCheckGPT-NLI proxy | 57.63 | 55.18 | 53.91 | 51.72 |
| SAPLMA (SAE-L18 MLP) | **97.64** | 82.67 | 68.43 | 55.89 |
| Concat-4L + PCA + LR | 95.82 | 85.31 | 72.16 | 58.43 |
| MuSAE-ERM (ablation) | 93.47 | 84.12 | 70.58 | 57.21 |
| **MuSAE-Inv (Ours)** | 92.12 | **89.53** | **78.26** | **63.17** |
| MuSAE-Inv (K=512) | 96.11 | 91.72 | 81.43 | 65.89 |

### Key Findings

- **Cross-domain robustness**: MuSAE-Inv loses only 2.6% AUROC from QAâ†’Dialogue, vs 15.0% drop for SAPLMA
- **Interpretability**: Only 0.78% of SAE features selected (512 Ã· 65,536), each monosemantic
- **Efficiency**: Trains in <2 min on CPU; inference â‰ˆ12ms per example
- **Statistical significance**: DeLong's test p < 0.001 vs all baselines on OOD domains

### AUROC Drop (ID â†’ OOD)

| Method | QAâ†’Dial | QAâ†’Summ | QAâ†’TQA |
|:---|:---:|:---:|:---:|
| SAPLMA | âˆ’14.97 | âˆ’29.21 | âˆ’41.75 |
| Concat-4L + PCA + LR | âˆ’10.51 | âˆ’23.66 | âˆ’37.39 |
| MuSAE-ERM (ablation) | âˆ’9.35 | âˆ’22.89 | âˆ’36.26 |
| **MuSAE-Inv (Ours)** | **âˆ’2.59** | **âˆ’13.86** | **âˆ’28.95** |

---

## Architecture

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚           Gemma-2-2B (2.6B params)       â”‚
                â”‚  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”         â”‚
Input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚ L6 â”‚  â”‚L12 â”‚  â”‚L18 â”‚  â”‚L25 â”‚         â”‚
                â”‚  â””â”€â”€â”¬â”€â”˜  â””â”€â”€â”¬â”€â”˜  â””â”€â”€â”¬â”€â”˜  â””â”€â”€â”¬â”€â”˜         â”‚
                â””â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚       â”‚       â”‚       â”‚
                      â–¼       â–¼       â–¼       â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Gemma Scope SAEs (16,384-width each)  â”‚
                â”‚   h â†’ áº‘ = ReLU(W_enc Â· h + b_enc)      â”‚
                â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚      â”‚      â”‚      â”‚
                   â–¼      â–¼      â–¼      â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  ICFS v2 Feature Selection       â”‚
            â”‚  score = min(|CE_QA|, |CE_Dial|) â”‚
            â”‚        Ã— sign_consistent         â”‚
            â”‚  â†’ Top-128 per layer â†’ 512 feat  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  MuSAE-Inv L1-LR Probe           â”‚
            â”‚  QA + Dialogue domain augment     â”‚
            â”‚  C=0.3, saga solver, balanced     â”‚
            â”‚  â†’ P(hallucination | x)           â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ICFS v2: Counterfactual-Based Feature Selection

The key innovation is ICFS v2 â€” a feature selection criterion that identifies **causally invariant** features:

1. **Generate counterfactual pairs**: For each example, extract SAE features for both the truthful and hallucinated response.
2. **Compute causal effect (CE)**: `CE_d[j] = mean(Î´[:, j])` where `Î´ = feat_true - feat_false` for domain `d`.
3. **Select invariant features**: `score[j] = min(|CE_QA[j]|, |CE_Dial[j]|) Ã— 1[sign(CE_QA[j]) == sign(CE_Dial[j])]`

This criterion ensures selected features have:
- **Large causal effect** on hallucination detection (min-CE)
- **Consistent direction** across domains (sign-consistency)

---

## Installation

### Prerequisites

- Python â‰¥ 3.10
- CUDA â‰¥ 11.8 (for GPU acceleration)
- ~6.5 GB VRAM (tested on NVIDIA Tesla P100-16GB)

### Quick Install

```bash
# Clone the repository
git clone https://github.com/vvinayakkk/MuSAE-Inv-Invariant-Causal-Feature-Selection-from-Sparse.git
cd MuSAE-Inv-Invariant-Causal-Feature-Selection-from-Sparse

# Create virtual environment (recommended)
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate   # Windows

# Install package and dependencies
pip install -e .

# Or install from requirements
pip install -r requirements.txt
```

### Development Install

```bash
pip install -e ".[dev]"
```

### Docker

```bash
docker build -t musae-inv:latest -f docker/Dockerfile .
docker run --gpus all -v $(pwd)/outputs:/app/outputs musae-inv:latest
```

### HuggingFace Token

Gemma-2 is a gated model. Set your HuggingFace token:

```bash
export HF_TOKEN="hf_your_token_here"
# Or create a .env file
echo "HF_TOKEN=hf_your_token_here" > .env
```

Get your token at [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).

> **Note**: You must accept the [Gemma license](https://huggingface.co/google/gemma-2-2b) on HuggingFace before downloading model weights.

---

## Quick Start

### 1. Download Datasets

```bash
python scripts/download_data.py
```

### 2. Run Full Pipeline

```bash
python scripts/train.py --config configs/default.yaml
```

### 3. Generate Figures

```bash
python scripts/generate_figures.py --config configs/default.yaml
```

### Minimal Example

```python
from musae_inv.config import Config
from musae_inv.models.model_loader import load_gemma_model, load_gemma_scope_saes
from musae_inv.features.extraction import extract_features
from musae_inv.features.icfs import compute_icfs_v2
from musae_inv.models.probes import MuSAEInvProbe

# Configuration
cfg = Config(icfs_top_k=128, musae_C=0.3)

# Load model + SAEs
model, tokenizer = load_gemma_model(cfg)
saes = load_gemma_scope_saes(cfg)

# Extract features
features = extract_features(df, model, tokenizer, saes, cfg.target_layers, cfg=cfg)

# Train probe
probe = MuSAEInvProbe(cfg)
probe.fit(X_train, y_train, X_dial_train, y_dial_train, X_val, y_val)

# Predict
p_hallucination = probe.predict_proba(X_test)
```

---

## Datasets

MuSAE-Inv uses four benchmarks spanning different hallucination types:

### HaluEval (Li et al., 2023)

A large-scale hallucination evaluation benchmark with ChatGPT-generated hallucinations:

| Split | Domain | Size | Source |
|:---|:---|:---:|:---|
| `qa_samples` | Question Answering | 10,000 | [HuggingFace](https://huggingface.co/datasets/pminervini/HaluEval) |
| `dialogue_samples` | Dialogue | 1,000 | [HuggingFace](https://huggingface.co/datasets/pminervini/HaluEval) |
| `summarization_samples` | Summarisation | 1,000 | [HuggingFace](https://huggingface.co/datasets/pminervini/HaluEval) |

- **Citation**: Li et al., "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models", EMNLP 2023
- **License**: MIT
- **Schema**: Each example contains a knowledge snippet, question/context, correct answer, and hallucinated answer

### TruthfulQA (Lin et al., 2022)

A benchmark measuring whether LLMs generate truthful answers:

| Split | Domain | Size | Source |
|:---|:---|:---:|:---|
| `multiple_choice` | General Knowledge | 817 | [HuggingFace](https://huggingface.co/datasets/truthfulqa/truthful_qa) |

- **Citation**: Lin et al., "TruthfulQA: Measuring How Models Mimic Human Falsehoods", ACL 2022
- **License**: Apache-2.0
- **Schema**: Questions with multiple-choice answers and truth labels

### Data Splits

```
HaluEval QA (10,000 examples):
  â”œâ”€â”€ Train: 7,000 (70%)
  â”œâ”€â”€ Val:   1,500 (15%)
  â””â”€â”€ Test:  1,500 (15%)

HaluEval Dialogue (1,000): â†’ Test only (OOD)
HaluEval Summarisation (1,000): â†’ Test only (OOD)
TruthfulQA (817): â†’ Test only (OOD)
```

### Downloading

```bash
# All datasets (auto-cached by HuggingFace)
python scripts/download_data.py

# Include model weights (requires HF_TOKEN)
HF_TOKEN=hf_xxx python scripts/download_data.py --include-model --include-saes
```

---

## Full Pipeline

### Step-by-Step Execution

```bash
# Step 1: Download datasets
python scripts/download_data.py

# Step 2: Extract SAE features (GPU required, ~45 min on P100)
python scripts/extract_features.py --config configs/default.yaml --counterfactual

# Step 3: Train MuSAE-Inv + all baselines
python scripts/train.py --config configs/default.yaml

# Step 4: Run baselines separately (optional)
python scripts/run_baselines.py --config configs/default.yaml

# Step 5: Evaluate
python scripts/evaluate.py --config configs/default.yaml --run-statistical-tests

# Step 6: Generate figures and tables
python scripts/generate_figures.py --config configs/default.yaml
python scripts/generate_tables.py --config configs/default.yaml
```

### Using Make (Linux/macOS)

```bash
make install        # Install dependencies
make download       # Download datasets
make train          # Run full training pipeline
make evaluate       # Evaluate with statistical tests
make figures        # Generate all 22 paper figures
make tables         # Generate CSV result tables
make all            # Run everything end-to-end
```

### Pipeline Outputs

```
outputs/
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ feature_cache.pkl          # SAE features for all splits (~2GB)
â”‚   â””â”€â”€ counterfactual_cache.pkl   # CF deltas for QA, Dial, Summ (~500MB)
â”‚   â””â”€â”€ icfs_cache_v2.pkl          # ICFS scores and indices
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ results_all.csv            # Full results matrix
â”‚   â”œâ”€â”€ results_brier_drop.csv     # Brier score degradation
â”‚   â”œâ”€â”€ results_auroc_drop.csv     # AUROC drop analysis
â”‚   â”œâ”€â”€ config.yaml                # Run configuration
â”‚   â””â”€â”€ tdv.npy                    # Truth direction vector
â””â”€â”€ plots/
    â”œâ”€â”€ fig01_main_results_heatmap.pdf
    â”œâ”€â”€ fig02_topk_ablation.pdf
    â”œâ”€â”€ ...
    â””â”€â”€ fig22_layer_contribution.pdf
```

---

## Configuration

All hyperparameters are managed via YAML configuration files:

```yaml
# configs/default.yaml
seed: 42
device: "cuda"

# Model
model_id: "google/gemma-2-2b"
torch_dtype: "bfloat16"
target_layers: [6, 12, 18, 25]
sae_width: 16384

# ICFS
icfs_top_k: 128         # Features per layer (128 Ã— 4 = 512 total)

# Probe
musae_C: 0.3            # L1 regularisation strength
musae_solver: "saga"    # Optimiser
musae_penalty: "l1"     # Sparsity
n_dial_train: 500       # Dialogue augmentation size
```

### Override via CLI

```bash
python scripts/train.py --config configs/default.yaml --icfs-top-k 512 --musae-C 1.0
```

### Custom Configuration

```python
from musae_inv.config import Config

cfg = Config(
    icfs_top_k=256,
    musae_C=0.1,
    target_layers=[12, 18, 25],
    output_dir="./my_experiment",
)
cfg.save("configs/my_config.yaml")
```

---

## Ablation Studies

### Top-K Ablation

Study the effect of feature selection sparsity:

```bash
make ablation-topk
# Or manually:
for K in 16 32 64 128 256 512 1024 2048 4096; do
    python scripts/train.py --config configs/ablation_topk.yaml --icfs-top-k $K
done
```

| K | QA AUROC | Dial AUROC | Sparsity |
|:---:|:---:|:---:|:---:|
| 16 | 81.23 | 79.41 | 0.10% |
| 64 | 89.76 | 86.92 | 0.39% |
| **128** | **92.12** | **89.53** | **0.78%** |
| 512 | 96.11 | 91.72 | 3.13% |
| 4096 | 97.83 | 88.14 | 25.00% |

### Layer Ablation

```bash
make ablation-layers
```

### Regularisation Sweep

```bash
make ablation-reg
# Sweeps C âˆˆ {0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10.0}
```

---

## Project Structure

```
MuSAE-Inv/
â”œâ”€â”€ musae_inv/                     # Core Python package
â”‚   â”œâ”€â”€ __init__.py                # Package metadata, version
â”‚   â”œâ”€â”€ config.py                  # Configuration dataclass + YAML I/O
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ model_loader.py        # Gemma-2-2B + SAE loading
â”‚   â”‚   â”œâ”€â”€ probes.py              # MuSAE-Inv, ERM, Single-layer, PCA probes
â”‚   â”‚   â””â”€â”€ saplma.py              # SAPLMA MLP baseline
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ datasets.py            # HaluEval + TruthfulQA loaders
â”‚   â”‚   â””â”€â”€ preprocessing.py       # ICFS feature building
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ extraction.py          # Hidden-state + SAE feature extraction
â”‚   â”‚   â”œâ”€â”€ counterfactual.py      # Counterfactual pair processing
â”‚   â”‚   â””â”€â”€ icfs.py                # ICFS v2 scoring algorithm
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ metrics.py             # AUROC, AUPRC, Bal-Acc, F1, Brier
â”‚   â”‚   â”œâ”€â”€ baselines.py           # SAE-entropy, TDV, baseline runners
â”‚   â”‚   â””â”€â”€ logit_lens.py          # Logit Lens trajectory consistency
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ mechanistic.py         # Layer statistics, PCA/t-SNE, overlap
â”‚   â”‚   â””â”€â”€ statistical.py         # DeLong, Hanley-McNeil CI, Cohen's d
â”‚   â”œâ”€â”€ visualization/
â”‚   â”‚   â”œâ”€â”€ plots.py               # 22 publication figures
â”‚   â”‚   â””â”€â”€ tables.py              # CSV result table generation
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ seed.py                # Reproducibility (seed everything)
â”‚       â””â”€â”€ io.py                  # Pickle/NumPy/JSON I/O
â”‚
â”œâ”€â”€ scripts/                       # Executable pipelines
â”‚   â”œâ”€â”€ train.py                   # Full end-to-end training
â”‚   â”œâ”€â”€ evaluate.py                # Evaluation with statistical tests
â”‚   â”œâ”€â”€ extract_features.py        # Feature extraction (GPU)
â”‚   â”œâ”€â”€ run_baselines.py           # All baseline methods
â”‚   â”œâ”€â”€ download_data.py           # Dataset download
â”‚   â”œâ”€â”€ generate_figures.py        # Paper figure generation
â”‚   â””â”€â”€ generate_tables.py         # Result table generation
â”‚
â”œâ”€â”€ configs/                       # YAML configuration files
â”‚   â”œâ”€â”€ default.yaml               # Standard configuration
â”‚   â”œâ”€â”€ ablation_topk.yaml         # Top-K ablation sweep
â”‚   â”œâ”€â”€ ablation_layers.yaml       # Layer ablation sweep
â”‚   â””â”€â”€ ablation_reg.yaml          # Regularisation sweep
â”‚
â”œâ”€â”€ tests/                         # Test suite (pytest)
â”‚   â”œâ”€â”€ conftest.py                # Shared fixtures
â”‚   â”œâ”€â”€ test_config.py             # Config tests
â”‚   â”œâ”€â”€ test_icfs.py               # ICFS algorithm tests
â”‚   â”œâ”€â”€ test_metrics.py            # Metric computation tests
â”‚   â”œâ”€â”€ test_preprocessing.py      # Data preprocessing tests
â”‚   â””â”€â”€ test_utils.py              # Utility function tests
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter notebooks
â”‚   â””â”€â”€ full_pipeline.ipynb        # Complete experimental notebook
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ INSTALLATION.md            # Detailed setup guide
â”‚   â”œâ”€â”€ USAGE.md                   # Usage examples
â”‚   â”œâ”€â”€ DATASETS.md                # Dataset documentation
â”‚   â”œâ”€â”€ METHODOLOGY.md             # Technical methodology
â”‚   â””â”€â”€ ARCHITECTURE.md            # Code architecture
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile                 # Reproducible container
â”‚
â”œâ”€â”€ paper/                         # LaTeX manuscript
â”‚   â”œâ”€â”€ main.tex                   # Full paper
â”‚   â””â”€â”€ references.bib             # Bibliography
â”‚
â”œâ”€â”€ pyproject.toml                 # Python packaging (PEP 621)
â”œâ”€â”€ requirements.txt               # Pinned dependencies
â”œâ”€â”€ requirements-dev.txt           # Development dependencies
â”œâ”€â”€ Makefile                       # Convenience targets
â”œâ”€â”€ CITATION.cff                   # Machine-readable citation
â”œâ”€â”€ CONTRIBUTING.md                # Contributing guidelines
â”œâ”€â”€ LICENSE                        # MIT License
â””â”€â”€ README.md                      # This file
```

---

## Reproducibility

### Exact Reproduction

All random seeds are deterministically set:

```python
# Seed configuration
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
```

### Environment

The experiments were run on:

| Component | Specification |
|:---|:---|
| **GPU** | NVIDIA Tesla P100-16GB (Kaggle) |
| **VRAM Usage** | ~6.44 GB peak |
| **Python** | 3.10.12 |
| **PyTorch** | 2.1.0+cu118 |
| **Transformers** | 4.44.2 |
| **SAE-Lens** | 4.4.x |
| **TransformerLens** | 2.7.x |
| **scikit-learn** | 1.5.x |

### Caching

Feature extraction is the most expensive step (~45 min on P100). All intermediate results are cached:

```
features/feature_cache.pkl          â†’ SAE features
features/counterfactual_cache.pkl   â†’ CF deltas
features/icfs_cache_v2.pkl          â†’ ICFS indices
```

Re-running the pipeline automatically uses cached results. Force recomputation:

```bash
python scripts/train.py --config configs/default.yaml  # Uses cache
# Edit config: force_recompute_feats: true              # Recomputes
```

---

## Hardware Requirements

| Stage | GPU | VRAM | Time |
|:---|:---:|:---:|:---:|
| Model + SAE loading | Required | ~5.2 GB | ~2 min |
| Feature extraction (all splits) | Required | ~6.4 GB | ~45 min |
| Counterfactual extraction | Required | ~6.4 GB | ~30 min |
| ICFS scoring | CPU only | â€” | ~5 sec |
| Probe training | CPU only | â€” | ~90 sec |
| Evaluation | CPU only | â€” | ~10 sec |
| Figure generation | CPU only | â€” | ~30 sec |

**Minimum**: 8 GB VRAM GPU (e.g., RTX 3060, T4, P100)  
**Recommended**: 16 GB VRAM (e.g., V100, RTX 4090, A100)  
**CPU-only**: Possible if you pre-extract features on a GPU instance and copy the cache

---

## Citation

If you use MuSAE-Inv in your research, please cite:

```bibtex
@article{musae_inv_2025,
  title={Multi-layer Sparse-Autoencoder Invariant Causal Feature Selection
         for Cross-Domain Hallucination Detection in Large Language Models},
  author={Vinayak Katoch},
  journal={arXiv preprint},
  year={2025},
  url={https://github.com/vvinayakkk/MuSAE-Inv-Invariant-Causal-Feature-Selection-from-Sparse}
}
```

---

## License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.

---

## Acknowledgements

- [Google DeepMind](https://deepmind.google/) for Gemma-2 and Gemma Scope SAEs
- [SAE-Lens](https://github.com/jbloomAus/SAELens) for SAE loading utilities
- [TransformerLens](https://github.com/TransformerLensOrg/TransformerLens) for mechanistic interpretability tools
- [HaluEval](https://github.com/RUCAIBox/HaluEval) benchmark authors
- [TruthfulQA](https://github.com/sylinrl/TruthfulQA) benchmark authors
- Kaggle for GPU compute resources

---

<div align="center">

**Built with â¤ï¸ for mechanistic interpretability research**

[Report Bug](https://github.com/vvinayakkk/MuSAE-Inv-Invariant-Causal-Feature-Selection-from-Sparse/issues) Â· [Request Feature](https://github.com/vvinayakkk/MuSAE-Inv-Invariant-Causal-Feature-Selection-from-Sparse/issues) Â· [Discussions](https://github.com/vvinayakkk/MuSAE-Inv-Invariant-Causal-Feature-Selection-from-Sparse/discussions)

</div>
